{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NN Template \u00b6 Generic template to bootstrap your PyTorch project. Click on and avoid writing boilerplate code for: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. DVC , track large files, directories, or ML models. Think \"Git for data\". Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. nn-template is opinionated so you don't have to be. If you use this template, please add to your README . Usage Examples \u00b6 Checkout the mwe branch to view a minimum working example on MNIST.","title":"Home"},{"location":"#nn-template","text":"Generic template to bootstrap your PyTorch project. Click on and avoid writing boilerplate code for: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. DVC , track large files, directories, or ML models. Think \"Git for data\". Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. nn-template is opinionated so you don't have to be. If you use this template, please add to your README .","title":"NN Template"},{"location":"#usage-examples","text":"Checkout the mwe branch to view a minimum working example on MNIST.","title":"Usage Examples"},{"location":"changelog/","text":"","title":"Changelog"},{"location":"upgrade/","text":"","title":"Upgrade"},{"location":"features/envvars/","text":"Environment Variables \u00b6 System specific variables (e.g. absolute paths to datasets) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from Python code with: get_env ( \"MY_VAR\" ) and in the Hydra .yaml configuration files with: ${oc.env:MY_VAR}","title":"Environment Variables"},{"location":"features/envvars/#environment-variables","text":"System specific variables (e.g. absolute paths to datasets) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from Python code with: get_env ( \"MY_VAR\" ) and in the Hydra .yaml configuration files with: ${oc.env:MY_VAR}","title":"Environment Variables"},{"location":"getting-started/generation/","text":"","title":"Generating your project"},{"location":"getting-started/structure/","text":"Structure \u00b6 . \u251c\u2500\u2500 .cache \u251c\u2500\u2500 conf # hydra compositional config \u2502 \u251c\u2500\u2500 nn \u2502 \u251c\u2500\u2500 default.yaml # current experiment configuration \u2502 \u251c\u2500\u2500 hydra \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 data # datasets \u251c\u2500\u2500 .env # system-specific env variables, e.g. PROJECT_ROOT \u251c\u2500\u2500 requirements.txt # basic requirements \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 common # common modules and utilities \u2502 \u251c\u2500\u2500 data # PyTorch Lightning datamodules and datasets \u2502 \u251c\u2500\u2500 modules # PyTorch Lightning modules \u2502 \u251c\u2500\u2500 run.py # entry point to run current conf \u2502 \u2514\u2500\u2500 ui # interactive streamlit apps \u2514\u2500\u2500 wandb # local experiments (auto-generated)","title":"Strucure"},{"location":"getting-started/structure/#structure","text":". \u251c\u2500\u2500 .cache \u251c\u2500\u2500 conf # hydra compositional config \u2502 \u251c\u2500\u2500 nn \u2502 \u251c\u2500\u2500 default.yaml # current experiment configuration \u2502 \u251c\u2500\u2500 hydra \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 data # datasets \u251c\u2500\u2500 .env # system-specific env variables, e.g. PROJECT_ROOT \u251c\u2500\u2500 requirements.txt # basic requirements \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 common # common modules and utilities \u2502 \u251c\u2500\u2500 data # PyTorch Lightning datamodules and datasets \u2502 \u251c\u2500\u2500 modules # PyTorch Lightning modules \u2502 \u251c\u2500\u2500 run.py # entry point to run current conf \u2502 \u2514\u2500\u2500 ui # interactive streamlit apps \u2514\u2500\u2500 wandb # local experiments (auto-generated)","title":"Structure"},{"location":"integrations/dvc/","text":"Data Version Control \u00b6 DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\" Making changes \u00b6 When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet Switching between versions \u00b6 The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Read more in the docs !","title":"DVC"},{"location":"integrations/dvc/#data-version-control","text":"DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\"","title":"Data Version Control"},{"location":"integrations/dvc/#making-changes","text":"When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet","title":"Making changes"},{"location":"integrations/dvc/#switching-between-versions","text":"The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Read more in the docs !","title":"Switching between versions"},{"location":"integrations/hydra/","text":"Hydra \u00b6 Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml . Sweeps \u00b6 You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: PYTHONPATH = . python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. We recommend to go through at least the Basic Tutorial , and the docs about Instantiating objects with Hydra .","title":"Hydra"},{"location":"integrations/hydra/#hydra","text":"Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml .","title":"Hydra"},{"location":"integrations/hydra/#sweeps","text":"You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: PYTHONPATH = . python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. We recommend to go through at least the Basic Tutorial , and the docs about Instantiating objects with Hydra .","title":"Sweeps"},{"location":"integrations/lightning/","text":"PyTorch Lightning \u00b6 Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/lightning/#pytorch-lightning","text":"Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/streamlit/","text":"Streamlit \u00b6 Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/streamlit/#streamlit","text":"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/wandb/","text":"Weights and Biases \u00b6 Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard. Quickstart \u00b6 Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log . W&B is our logger of choice, but that is a purely subjective decision. Since we are using Lightning, you can replace wandb with the logger you prefer (you can even build your own). More about Lightning loggers here .","title":"Weigth & Biases"},{"location":"integrations/wandb/#weights-and-biases","text":"Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard.","title":"Weights and Biases"},{"location":"integrations/wandb/#quickstart","text":"Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log . W&B is our logger of choice, but that is a purely subjective decision. Since we are using Lightning, you can replace wandb with the logger you prefer (you can even build your own). More about Lightning loggers here .","title":"Quickstart"}]}